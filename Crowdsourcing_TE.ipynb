{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import csv\n",
    "from AI_Responder_Settings import responder_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import AI_Responder_Settings\n",
    "importlib.reload(AI_Responder_Settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"openai_key.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "openai_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ai_gpt(\n",
    "        model_name, temp, instructions, input_prompt\n",
    "        ):\n",
    "    client = openai.OpenAI(api_key = openai_key)\n",
    "    completion = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": input_prompt}\n",
    "    ],\n",
    "    temperature = temp,\n",
    "    response_format = { \"type\": \"json_object\" }\n",
    "    )\n",
    "\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate = \"\"\"\n",
    "Simulate the responses of 5 independent Amazon Mechanical Turk workers answering a given task. \n",
    "Assume each worker will attempt to provide some answer to the best of their ability despite a lack of experience or knowledge. \"\"\"\n",
    "\n",
    "formatting = \"\"\"\n",
    "\n",
    "Making sure NOT to include preceding string that says \"json\"; output ONLY a json in the following format:\n",
    "\n",
    "{\n",
    "    \"responses\": [\n",
    "        {\n",
    "            \"confidence\": 3,\n",
    "            \"reason\": \"...\",\n",
    "            \"answer\": \"...\"\n",
    "        },\n",
    "        {\n",
    "            ...\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "intro = \"\"\"\n",
    "Introduction: Many individuals with autism visit online communities\n",
    "to ask questions about issues they are facing. We would like to know\n",
    "whether people outside of the specific community could provide\n",
    "answers for them. A question from a special autism community will\n",
    "be shown to you. Please read the question carefully, rate your\n",
    "confidence and provide your own answer to the question.\n",
    "\"\"\"\n",
    "\n",
    "tasks = \"\"\"\n",
    "Tasks:\n",
    "1. Rate your confidence in being able to answer this question.\n",
    "(1) Not confident at all - (2) Slightly confident -\n",
    "(3) Somewhat confident - (4) Very confident -\n",
    "(5) Extremely confident\n",
    "2. Explain the reason for your confidence rating:\n",
    "3. Please answer the question, or explain why you cannot answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions = []\n",
    "with open('questions.csv', mode ='r') as file:    \n",
    "       csvFile = csv.DictReader(file)\n",
    "       for lines in csvFile:\n",
    "              all_questions.append(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_answers = {}\n",
    "for setting in responder_settings[3:]:\n",
    "    name = setting[\"setting_name\"]\n",
    "    ai_answers[name] = {}\n",
    "    for question in all_questions[0:6]:\n",
    "        scenario = f\"\"\"\n",
    "        Question Title: {question[\"Title\"]}\n",
    "        Question Body: {question[\"Body\"]}\n",
    "    \"\"\"\n",
    "\n",
    "        instructions = simulate + setting[\"additional_information\"] + setting[\"token_limit\"] + formatting\n",
    "        prompt =  intro + scenario + tasks \n",
    "\n",
    "        raw_out = call_ai_gpt(setting[\"model\"], setting[\"temp\"], instructions, prompt)\n",
    "        resp = raw_out.choices[0].message.content\n",
    "        response = json.loads(resp)[\"responses\"]\n",
    "        for answer in response:\n",
    "            ai_answers[name].setdefault(question[\"Body\"], []).append(answer[\"answer\"])\n",
    "\n",
    "with open('ai_answers.json', 'w') as f:\n",
    "    json.dump(ai_answers, f, ensure_ascii=False, indent=4)\n",
    "print(json.dumps(ai_answers, ensure_ascii=False, indent=4))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outgroup_answers = {}\n",
    "with open('out-group_answers.csv', mode ='r') as file:    \n",
    "       csvFile = csv.DictReader(file)\n",
    "       for lines in csvFile:\n",
    "              outgroup_answers.setdefault(lines[\"Input.body\"], []).append(lines[\"Answer.answer\"])\n",
    "\n",
    "print(json.dumps(outgroup_answers, indent=4))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ai_answers.json', 'r') as file:\n",
    "    ai_answers = json.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro = \"\"\"\n",
    "You are simulating the role of a rater: you are given a question below and your task is to rate a response to the question according to several criteria. \n",
    "Making sure NOT to include any preceding string that says json, output ONLY a json of a dictionary with each criteria and its rating being a key-value pair according to the below format:\n",
    "\n",
    "{\n",
    "  \"Helpfulness\": 4,\n",
    "  \"Emotional support\": \"Yes\",\n",
    "  \"Directness\": \"Yes\",\n",
    "  \"Additional Information Provided\": \"No\",\n",
    "  \"Justification\": \"...\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "tasks = \"\"\"\n",
    "Rate this response using the following criteria:\n",
    "Helpfulness (1-5) scale: How effectively does the response address the question or provide useful advice?\n",
    "Emotional support = Yes/No: Does the response convey empathy or offer emotional reassurance?\n",
    "Directness (Yes/No) is the repsonse clear and straightforward?\n",
    "Additional Information Provided (Yes/No): Does the response include any extra information that could be useful?\n",
    "Also provide a justification for the above ratings.\n",
    "\"\"\"\n",
    "answers = ai_answers\n",
    "questions_length = 5\n",
    "responses_length = 5\n",
    "divisor = questions_length * responses_length\n",
    "\n",
    "setting_names = []\n",
    "ratings = []\n",
    "metrics = [\"Helpfulness\", \"Emotional support\", \"Directness\", \"Additional info\"]\n",
    "\n",
    "for i, setting in enumerate(answers.keys()):\n",
    "  select_questions = list(answers[setting].keys())[0:questions_length]\n",
    "  helpfulness, emotional_support, directness, additional_info = 0, 0, 0, 0\n",
    "  for j, question in enumerate(select_questions):\n",
    "    for response in answers[setting][question]:\n",
    "        prompt_question = \"\\nConsider the question: \" + question\n",
    "        prompt_response = \"\\nNow consider the following response: \" + response\n",
    "        prompt = prompt_question + prompt_response + tasks\n",
    "\n",
    "        raw_out = call_ai_gpt(\"gpt-4.1-mini\", 1, intro, prompt)\n",
    "        response = json.loads(raw_out.choices[0].message.content)\n",
    "\n",
    "        print(response)\n",
    "\n",
    "        helpfulness += int(response[\"Helpfulness\"])\n",
    "        emotional_support = (emotional_support + 1) if response[\"Emotional support\"] == \"Yes\" else emotional_support\n",
    "        directness = (directness + 1) if response[\"Directness\"] == \"Yes\" else directness\n",
    "        additional_info = (additional_info + 1) if response[\"Additional Information Provided\"] == \"Yes\" else additional_info\n",
    "\n",
    "  helpfulness /= divisor\n",
    "  emotional_support /= divisor\n",
    "  directness /= divisor\n",
    "  additional_info /= divisor\n",
    "\n",
    "  curr_rating = [helpfulness, emotional_support, directness, additional_info]\n",
    "\n",
    "\n",
    "  setting_names.append(setting)\n",
    "  ratings.append(curr_rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(setting_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"Helpfulness\", \"Emotional support\", \"Directness\", \"Additional info\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[3.6, 0.6, 1.0, 0.8], [3.52, 0.72, 1.0, 0.68]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_ratings = np.array(ratings)\n",
    "converted_ratings[:, 0] = converted_ratings[:, 0] / 5\n",
    "print(converted_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(18, 6))\n",
    "axes = axes.flatten()  # flatten to 1D array to loop over easily\n",
    "\n",
    "# Normalize across all data to keep color scale consistent\n",
    "norm = mcolors.Normalize(vmin=np.min(converted_ratings), vmax=np.max(converted_ratings))\n",
    "cmap = cm.get_cmap('Reds')  # or any sequential colormap\n",
    "\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    values = converted_ratings[i]\n",
    "    colors = cmap(norm(values))  # Get color for each bar based on height\n",
    "\n",
    "    bars = ax.bar(metrics, converted_ratings[i], color=colors)\n",
    "    ax.set_title(setting_names[i])\n",
    "    ax.set_ylim(0, 1.15)  # consistent scale across plots\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Add text labels above each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            height + 0.01,  # small padding above the bar\n",
    "            f'{height:.2f}',  # format number\n",
    "            ha='center', va='bottom', fontsize=8\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
